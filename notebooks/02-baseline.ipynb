{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <function _WandbInit._resume_backend at 0x7f82dcf01790> (for pre_run_cell):\n"
     ]
    },
    {
     "ename": "BrokenPipeError",
     "evalue": "[Errno 32] Broken pipe",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBrokenPipeError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m~/opt/miniconda3/envs/gzaff/lib/python3.8/site-packages/backcall/backcall.py:104\u001b[0m, in \u001b[0;36madapted\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/gzaff/lib/python3.8/site-packages/wandb/sdk/wandb_init.py:399\u001b[0m, in \u001b[0;36m_resume_backend\u001b[0;34m(self)\u001b[0m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/gzaff/lib/python3.8/site-packages/wandb/sdk/interface/interface.py:650\u001b[0m, in \u001b[0;36mInterfaceBase.publish_resume\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    648\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpublish_resume\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    649\u001b[0m     resume \u001b[39m=\u001b[39m pb\u001b[39m.\u001b[39mResumeRequest()\n\u001b[0;32m--> 650\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_publish_resume(resume)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/gzaff/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py:314\u001b[0m, in \u001b[0;36m_publish_resume\u001b[0;34m(self, resume)\u001b[0m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/gzaff/lib/python3.8/site-packages/wandb/sdk/interface/interface_sock.py:51\u001b[0m, in \u001b[0;36m_publish\u001b[0;34m(self, record, local)\u001b[0m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/gzaff/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py:221\u001b[0m, in \u001b[0;36msend_record_publish\u001b[0;34m(self, record)\u001b[0m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/gzaff/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py:155\u001b[0m, in \u001b[0;36msend_server_request\u001b[0;34m(self, msg)\u001b[0m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/gzaff/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py:152\u001b[0m, in \u001b[0;36m_send_message\u001b[0;34m(self, msg)\u001b[0m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/gzaff/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py:130\u001b[0m, in \u001b[0;36m_sendall_with_error_handle\u001b[0;34m(self, data)\u001b[0m\n",
      "\u001b[0;31mBrokenPipeError\u001b[0m: [Errno 32] Broken pipe"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <function _WandbInit._pause_backend at 0x7f82dcf01550> (for post_run_cell):\n"
     ]
    },
    {
     "ename": "BrokenPipeError",
     "evalue": "[Errno 32] Broken pipe",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBrokenPipeError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m~/opt/miniconda3/envs/gzaff/lib/python3.8/site-packages/backcall/backcall.py:104\u001b[0m, in \u001b[0;36madapted\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/gzaff/lib/python3.8/site-packages/wandb/sdk/wandb_init.py:394\u001b[0m, in \u001b[0;36m_pause_backend\u001b[0;34m(self)\u001b[0m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/gzaff/lib/python3.8/site-packages/wandb/sdk/interface/interface.py:642\u001b[0m, in \u001b[0;36mInterfaceBase.publish_pause\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    640\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpublish_pause\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    641\u001b[0m     pause \u001b[39m=\u001b[39m pb\u001b[39m.\u001b[39mPauseRequest()\n\u001b[0;32m--> 642\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_publish_pause(pause)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/gzaff/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py:310\u001b[0m, in \u001b[0;36m_publish_pause\u001b[0;34m(self, pause)\u001b[0m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/gzaff/lib/python3.8/site-packages/wandb/sdk/interface/interface_sock.py:51\u001b[0m, in \u001b[0;36m_publish\u001b[0;34m(self, record, local)\u001b[0m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/gzaff/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py:221\u001b[0m, in \u001b[0;36msend_record_publish\u001b[0;34m(self, record)\u001b[0m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/gzaff/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py:155\u001b[0m, in \u001b[0;36msend_server_request\u001b[0;34m(self, msg)\u001b[0m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/gzaff/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py:152\u001b[0m, in \u001b[0;36m_send_message\u001b[0;34m(self, msg)\u001b[0m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/gzaff/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py:130\u001b[0m, in \u001b[0;36m_sendall_with_error_handle\u001b[0;34m(self, data)\u001b[0m\n",
      "\u001b[0;31mBrokenPipeError\u001b[0m: [Errno 32] Broken pipe"
     ]
    }
   ],
   "source": [
    "\n",
    "import lightgbm as lg\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import average_precision_score\n",
    "import numpy as np\n",
    "import wandb\n",
    "from wandb.lightgbm import wandb_callback, log_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('/Users/favea/Downloads/swiss-data/train_df_balanced_0.8.csv' ,index_col=0)\n",
    "train_df = train_df.drop(['delay_minutes','scheduled_time_departure','year'],axis=1)\n",
    "x_train, y_train = train_df.drop('delayed',axis=1), train_df.loc[:,'delayed']\n",
    "x_train.carrier = x_train.carrier.astype('category')\n",
    "x_train.origin = x_train.origin.astype('category')\n",
    "x_train.destination = x_train.destination.astype('category')\n",
    "x_train.previous_is_delayed = x_train.previous_is_delayed.astype('category')\n",
    "\n",
    "lgb_train = lgb.Dataset(x_train, y_train, categorical_feature = ['carrier','origin','destination','previous_is_delayed'], free_raw_data=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_df = pd.read_csv('/Users/favea/Downloads/swiss-data/dev_df.csv', index_col=0)\n",
    "dev_df = dev_df.drop(['Unnamed: 0','delay_minutes','scheduled_time_departure','year'],axis=1)\n",
    "\n",
    "x_dev, y_dev = dev_df.drop('delayed',axis=1), dev_df.loc[:,'delayed']\n",
    "x_dev.carrier = x_dev.carrier.astype('category')\n",
    "x_dev.origin = x_dev.origin.astype('category')\n",
    "x_dev.destination = x_dev.destination.astype('category')\n",
    "x_dev.previous_is_delayed = x_dev.previous_is_delayed.astype('category')\n",
    "\n",
    "\n",
    "lgb_dev = lgb.Dataset(x_dev, y_dev, reference=lgb_train, categorical_feature = ['carrier','origin','destination','previous_is_delayed'], free_raw_data=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Train baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify your configurations as a dict\n",
    "params = {\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'binary',\n",
    "    'metric': ['binary_logloss','auc','average_precision'],\n",
    "    'num_leaves': 50,\n",
    "    'learning_rate': 0.05,\n",
    "    'feature_fraction': 0.9,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'verbosity': 1,\n",
    "    'scale_pos_weight': 1,\n",
    "}\n",
    "\n",
    "wandb.init(project='swiss-delay-prediction', job_type= 'training-lightgbm-baseline-balanced', config=params);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm = lgb.train(params,\n",
    "                lgb_train,\n",
    "                num_boost_round=500,\n",
    "                valid_sets=lgb_dev,\n",
    "                valid_names=('validation'),\n",
    "                callbacks=[wandb_callback()],\n",
    "                early_stopping_rounds=20,\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_summary(gbm, save_model_checkpoint=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report, roc_auc_score, f1_score, precision_recall_fscore_support, roc_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict\n",
    "y_pred = gbm.predict(x_dev, num_iteration=gbm.best_iteration)\n",
    "\n",
    "roc_ac = roc_auc_score(y_dev, y_pred)\n",
    "wandb.log({'roc_auc': roc_ac})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.sklearn.plot_confusion_matrix(y_dev, (y_pred>=0.5).astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = gbm.predict(x_train, num_iteration=gbm.best_iteration)\n",
    "print(classification_report(y_dev, (y_pred>=0.5).astype(int)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = classification_report(y_dev, (y_pred>=0.5).astype(int), output_dict=True)\n",
    "sensitivity = d['1']['recall']\n",
    "specificity = d['0']['recall']\n",
    "f1_score = d['weighted avg']['f1-score']\n",
    "\n",
    "wandb.summary['sensitivity'] = sensitivity\n",
    "wandb.summary['specificity'] = specificity\n",
    "wandb.summary['f1_score'] = f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_positive_rate, true_positive_rate, thresholds = metrics.roc_curve(y_dev, y_pred) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import RocCurveDisplay, PrecisionRecallDisplay\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display = PrecisionRecallDisplay.from_predictions(    \n",
    "    y_dev,\n",
    "    y_pred,\n",
    "    color=\"darkorange\",\n",
    "    name = 'LightGBM'\n",
    ")\n",
    "plt.plot()\n",
    "wandb.log({'PR-curve':wandb.Image(plt)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RocCurveDisplay.from_predictions(\n",
    "    y_dev,\n",
    "    y_pred,\n",
    "    color=\"darkorange\",\n",
    ")\n",
    "plt.plot([0, 1], [0, 1], \"k--\", label=\"chance level (AUC = 0.5)\")\n",
    "plt.axis(\"square\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "wandb.log({'ROC-curve':wandb.Image(plt)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SWEEP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "  \"method\" : \"bayes\",\n",
    "  \"metric\": {\n",
    "      \"name\": \"avg_precision_dev\",\n",
    "      \"goal\": \"maximize\"\n",
    "  },\n",
    "  \"parameters\" : {\n",
    "    \"learning_rate\" :{\n",
    "      \"min\": 0.001,\n",
    "      \"max\": 1.0\n",
    "    },\n",
    "    \"gamma\" :{\n",
    "      \"min\": 0.001,\n",
    "      \"max\": 1.0\n",
    "    },\n",
    "    \"min_child_weight\" :{\n",
    "      \"min\": 1,\n",
    "      \"max\": 150\n",
    "    },\n",
    "    \"early_stopping_rounds\" :{\n",
    "      \"values\" : [10, 30, 40]\n",
    "    },\n",
    "    \"boosting_type\":{\"values\":['gbdt','dart']},\n",
    "    \"num_leaves\":{\"values\":[40,50, 60]},\n",
    "    \"feature_fraction\": {\"values\":[0.4,0.9,1]},\n",
    "    \"bagging_fraction\":{\"values\":[0.5,0.8,1]},\n",
    "    \"bagging_freq\":{'values':[1,5,10]},\n",
    "    \"scale_pos_weight\":{\"values\":[1, 0.8]},\n",
    "    \"num_iterations\":{\"values\":[600,1000, 2000,3000]},\n",
    "    \"lambda_l1\":{\"values\":[0.0, 0.3,0.5, 0.8]},\n",
    "    \"lambda_l2\":{\"values\":[0.0, 0.3,0.5, 0.8]},\n",
    "\n",
    "  }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():     \n",
    "    with wandb.init() as run:\n",
    "\n",
    "        params = {\n",
    "            'boosting_type': run.config['boosting_type'],\n",
    "            'objective': 'binary',\n",
    "            'metric': ['binary_logloss','auc','average_precision'],\n",
    "            'num_leaves': run.config['num_leaves'],\n",
    "            'learning_rate': run.config['learning_rate'],\n",
    "            'feature_fraction': run.config['feature_fraction'],\n",
    "            'bagging_fraction': run.config['bagging_fraction'],\n",
    "            'bagging_freq': run.config['bagging_freq'],\n",
    "            'verbosity': 1,\n",
    "            'scale_pos_weight': run.config['scale_pos_weight'],\n",
    "            'lambda_l2':run.config['lambda_l2'],\n",
    "            'lambda_l1':run.config['lambda_l1'],\n",
    "\n",
    "        }\n",
    "    \n",
    "\n",
    "        # Initialize and train LightGBM model\n",
    "        gbm = lgb.train(params,\n",
    "                        lgb_train,\n",
    "                        num_boost_round=run.config['num_iterations'],\n",
    "                        valid_sets=lgb_dev,\n",
    "                        valid_names=('validation'),\n",
    "                        callbacks=[wandb_callback()],\n",
    "                        early_stopping_rounds=10,\n",
    "                        )\n",
    "\n",
    "        log_summary(gbm, save_model_checkpoint=True)\n",
    "        \n",
    "\n",
    "\n",
    "        # Log booster metrics\n",
    "        run.summary[\"best_score\"] = gbm.best_score\n",
    "        run.summary[\"best_iteration\"] = gbm.best_iteration\n",
    "        \n",
    "        # Get train and validation predictions\n",
    "        y_dev_pred = gbm.predict(x_dev, num_iteration=gbm.best_iteration)\n",
    "        y_train_pred = gbm.predict(x_train, num_iteration=gbm.best_iteration)\n",
    "\n",
    "\n",
    "        # Log additional Train metrics\n",
    "        false_positive_rate, true_positive_rate, thresholds = metrics.roc_curve(y_train, y_train_pred) \n",
    "\n",
    "        avg_precision_train = average_precision_score(y_train, y_train_pred)\n",
    "        run.summary['train_avg_precision'] = avg_precision_train\n",
    "        run.summary['train_ks_stat'] = max(true_positive_rate - false_positive_rate)\n",
    "        run.summary['train_auc'] = metrics.auc(false_positive_rate, true_positive_rate)\n",
    "        run.summary['train_log_loss'] = -(y_train * np.log(y_train_pred) + (1-y_train) * np.log(1-y_train_pred)).sum() / len(y_train)\n",
    "\n",
    "        # Log additional Validation metrics\n",
    "        avg_precision_dev = average_precision_score(y_dev, y_dev_pred)\n",
    "        run.summary['avg_precision_dev'] = avg_precision_dev\n",
    "        run.summary[\"val_auc\"] = metrics.roc_auc_score(y_dev, y_dev_pred)\n",
    "        run.summary[\"val_acc_0.5\"] = metrics.accuracy_score(y_dev, np.where(y_dev_pred >= 0.5, 1, 0))\n",
    "        run.summary[\"val_log_loss\"] = -(y_dev * np.log(y_dev_pred) \n",
    "                                             + (1-y_dev) * np.log(1-y_dev_pred)).sum() / len(y_dev)\n",
    "\n",
    "        d = classification_report(y_dev, np.where(y_dev_pred >= 0.5, 1, 0), output_dict=True)\n",
    "        sensitivity = d['1']['recall']\n",
    "        specificity = d['0']['recall']\n",
    "        f1_score = d['weighted avg']['f1-score']\n",
    "\n",
    "        run.summary['val_sensitivity_0.5'] = sensitivity\n",
    "        run.summary['val_specificity_0.5'] = specificity\n",
    "        run.summary['val_f1_score_0.5'] = f1_score\n",
    "\n",
    "\n",
    "        # Log plots\n",
    "        RocCurveDisplay.from_predictions(\n",
    "            y_dev,\n",
    "            y_dev_pred,\n",
    "            color=\"darkorange\",\n",
    "        )\n",
    "        plt.plot([0, 1], [0, 1], \"k--\", label=\"chance level (AUC = 0.5)\")\n",
    "        plt.axis(\"square\")\n",
    "        plt.xlabel(\"False Positive Rate\")\n",
    "        plt.ylabel(\"True Positive Rate\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "        run.log({'ROC-curve':wandb.Image(plt)})\n",
    "\n",
    "        display = PrecisionRecallDisplay.from_predictions(    \n",
    "            y_dev,\n",
    "            y_dev_pred,\n",
    "            color=\"darkorange\",\n",
    "            name = 'LightGBM'\n",
    "        )\n",
    "        plt.plot()\n",
    "        plt.xlabel(\"Precision\")\n",
    "        plt.ylabel(\"Recall\")\n",
    "        plt.legend()\n",
    "        run.log({'PR-curve':wandb.Image(plt)})\n",
    "\n",
    "\n",
    "        #run.log({'feature_importance':fig})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_id = wandb.sweep(sweep_config, project=\"swiss-delay-prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 50 # number of runs to execute\n",
    "wandb.agent(sweep_id, function=train, count=count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For heavily unbalanced datasets such as 1:10000:\n",
    "\n",
    "max_bin: keep it only for memory pressure, not to tune (otherwise overfitting)\n",
    "\n",
    "learning rate: keep it only for training speed, not to tune (otherwise overfitting)\n",
    "\n",
    "n_estimators: must be infinite (like 9999999) and use early stopping to auto-tune (otherwise overfitting)\n",
    "\n",
    "num_leaves: [7, 4095]\n",
    "\n",
    "max_depth: [2, 63] and infinite (I personally saw metric performance increases with such 63 depth with small number of leaves on sparse unbalanced datasets)\n",
    "\n",
    "scale_pos_weight: [1, 10000] (if over 10000, something might be wrong because I never saw it that good after 5000)\n",
    "\n",
    "min_child_weight: [0.01, (sample size / 1000)] if you are using logloss (think about the hessian possible value rang\n",
    "e before putting \"sample size / 1000\", it is dataset-dependent and loss-dependent)\n",
    "\n",
    "\n",
    "subsample: [0.4, 1]\n",
    "\n",
    "\n",
    "bagging_freq: only 1, keep as is (otherwise overfitting)\n",
    "\n",
    "colsample_bytree: [0.4, 1]\n",
    "\n",
    "is_unbalance: false (make your own weighting with scale_pos_weight)\n",
    "\n",
    "USE A CUSTOM METRIC (to reflect reality without weighting, otherwise you have weights inside your metric with premade metrics like xgboost)\n",
    "\n",
    "Never tune these parameters unless you have an explicit requirement to tune them:\n",
    "\n",
    "Learning rate (lower means longer to train but more accurate, higher means smaller to train but less accurate)\n",
    "\n",
    "Number of boosting iterations (automatically tuned with early stopping and learning rate)\n",
    "\n",
    "Maximum number of bins (RAM dependent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('gzaff')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "280bb2b5bdab4e32d0e32ca5e48973259a6d8e2f8db8c3a8a31b6fc317ce4513"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
