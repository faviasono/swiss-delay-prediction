{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "DATA_PATH ='/Users/favea/Downloads/swiss-data'\n",
    "import pandas as pd\n",
    "import wandb\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.set(style='whitegrid', palette=\"deep\", font_scale=1.1, rc={\"figure.figsize\": [8, 5]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_delay = pd.read_csv(os.path.join(DATA_PATH,'delay.csv'), delimiter=';', dtype={'wh_fdel_delay_subcode':str})\n",
    "df_fis = pd.read_csv(os.path.join(DATA_PATH,'fis.csv'), delimiter=';')\n",
    "merged_df = df_fis.merge(df_delay, left_on='wh_fleg_leg_i', right_on='wh_fdel_leg_i')\n",
    "\n",
    "merged_df['is_delayed'] = merged_df.wh_fdel_delay_time>=15 # We define delay as 15 miutes threhold of delay time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_merged = merged_df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's check the reference label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can already see that the labels are not very balanced, it might be a problem during training.\n",
    "sns.displot(\n",
    "    merged_df['wh_fdel_delay_time'], kde=False, bins=100,\n",
    ").set(xlabel='Delay', ylabel='Count');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.groupby('is_delayed').count().iloc[:,0] # 5:1 the proportion of Not-delayed and delayed  (every 5 flight, there's one that's delayed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Airport factors\n",
    "\n",
    "1. We want to add information about number of flights departing on that airport that day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.wh_fleg_dep_dt_scd = pd.to_datetime(merged_df.wh_fleg_dep_dt_scd )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights_per_day = merged_df.groupby(by=['wh_fleg_dep_day_scd','wh_fleg_dep_ap_scd']).count().reset_index().iloc[:,:3].rename(columns ={'wh_fleg_leg_i':'flights_per_day'} )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.merge(left = merged_df, right = flights_per_day, on= ['wh_fleg_dep_day_scd','wh_fleg_dep_ap_scd'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df[['flights_per_day','wh_fdel_delay_time']].corr() # does not seem correlated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. We want information about flights departing at the same time of the flight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_flights_within_one_hour(row):\n",
    "    # Filter the DataFrame to only include rows with a departure time within one hour of the input datetime\n",
    "    start_time = row[\"wh_fleg_dep_dt_scd\"] - pd.Timedelta(hours=1)\n",
    "    end_time = row[\"wh_fleg_dep_dt_scd\"] + pd.Timedelta(hours=1)\n",
    "    airport = row[\"wh_fleg_dep_ap_scd\"]\n",
    "    filtered_data = merged_df[(merged_df[\"wh_fleg_dep_dt_scd\"] >= start_time) & (merged_df[\"wh_fleg_dep_dt_scd\"] <= end_time) & (merged_df['wh_fleg_dep_ap_scd']==airport)]\n",
    "    \n",
    "    # Count the number of rows in the filtered DataFrame\n",
    "    return filtered_data.count()['wh_fleg_leg_i']\n",
    "    \n",
    "from multiprocessing import cpu_count, pool\n",
    "num_processes = cpu_count()\n",
    "pool = pool.Pool(num_processes)\n",
    "results = [pool.apply_async(count_flights_within_one_hour, (row,)) for index, row in merged_df.iterrows()]\n",
    "\n",
    "# Wait for the parallel processing to finish\n",
    "pool.close()\n",
    "pool.join()\n",
    "\n",
    "# Store the results of the parallel processing in a list\n",
    "num_flights = [result.get() for result in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.to_csv('/Users/favea/Downloads/swiss-data/10122022_merged.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights_within_hour = pd.read_csv('/Users/favea/Downloads/swiss-data/num_flights_within_hour.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df['flights_btw_1_hour'] = flights_within_hour"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Info about previous flight on the wheel \n",
    "\n",
    "If a previous flight on the same wheel is delayed, it's more likely i'll have a delay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "previous_delays = merged_df[['wh_fleg_leg_i','is_delayed']].copy().rename(columns={'wh_fleg_leg_i':'previous_flight','is_delayed':'previous_is_delayed'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.merge(left = merged_df, right = previous_delays, how='left', left_on = 'wh_fleg_rot_leg_i_prev', right_on = 'previous_flight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.groupby(by=['previous_is_delayed','is_delayed']).count() # it makes sense indeed that if a previous flight has been delayed, we'll have a delay on the current one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# there will be NaN values, we need to address that.\n",
    "# Let's chedck how many have delayed label (if not too much, we can remove them altogheter)\n",
    "merged_df.previous_is_delayed.isna().sum()\n",
    "merged_df[merged_df.previous_is_delayed.isna()].groupby(by='is_delayed').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "About 20% have delayed = True.\n",
    "We can remove it, or we can add False if a NaN is present in previous_is_delayed (Meaning we assume no delay on preivous rotation)\n",
    "\n",
    "Let's remove it: it can help with imbalance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = merged_df[~merged_df.previous_is_delayed.isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = merged_df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering the dataset\n",
    "\n",
    "### Positioning/Turnaround flights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 125 flights that have departure and arrival in the same location.\n",
    "The reason might be that these flights are CARRY or POSITIONING flights, not used to carry people or supplies.\n",
    "Intuitively, delays should be detected also for this kind of flights, however the impact on the business and customer satisfaction is not that relevant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "positioning_flights = merged_df[merged_df.wh_fleg_dep_ap_scd == merged_df.wh_fleg_arr_ap_scd]\n",
    "positioning_flights.head(10).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check how many of these positioning flights are actually late\n",
    "positioning_flights.groupby(by='is_delayed').count().iloc[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the presence of many delayed flights, it might be important to consider them and use to train our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cancelled Flights\n",
    "If there's cancelled fligths, I can remove them since they can't be considered delayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.groupby('wh_fleg_leg_state').count().iloc[:,0] # There's 4254 cancelled flights out of 262094"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.groupby(['wh_fleg_leg_state','is_delayed']).count().iloc[:,0] # There's only 16 flights among the cancelled flights that are labeled as 'delayed'. Therefore we can remove it.\n",
    "\n",
    "cancelled_flights_idx  = merged_df.wh_fleg_leg_state == 'CNL'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = merged_df[~cancelled_flights_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features that might be useful.\n",
    "\n",
    "I want to consider only features that do not introduce an excessive bias during inference. Namely, if I insert scheduled departure time and airborne departure time, \n",
    "it's likely that my model will be able to predict pretty well whether there's gonna be a delay. \n",
    "Hence, I want to consider only features about:\n",
    "\n",
    "* Flights ( ORIGIN, DESTINATION, LENGTH OF THE TRIP, HOUR_OF_TRIP, DAY_OF_TRIP, SEASON_OF_TRIP, #LEGS (I.E. LAYOVERS + 1))\n",
    "* MAIL/CARGO (information about loading/off-loading of cargo items or mail that can delay the departure) \n",
    "* LUGGAGES (the more the luggages, the more time is needed to load them in the airplane. Also, their weight might correlated with delay)\n",
    "* PASSENGERS \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_columns = { # FLIGHT INFO\n",
    "                'wh_fleg_leg_i':'id_flight',\n",
    "                'wh_fleg_flt_carrier': 'carrier',\n",
    "                'wh_fleg_dep_ap_scd': 'origin',\n",
    "                'wh_fleg_arr_ap_scd': 'destination',\n",
    "                'wh_fleg_leg_dist_scd': 'distance_trip',\n",
    "                'wh_fleg_season':'season_trip',\n",
    "                'wh_fleg_dep_dt_scd':'scheduled_time_departure', # to parse (DAY | MONTH | HOUR)\n",
    "                \n",
    "\n",
    "                \n",
    "                # MAIL/CARGO INFO\n",
    "                'wh_fleg_mail':'mails_data',\n",
    "                'wh_fleg_cargo':'cargo_data',\n",
    "                \n",
    "                # Luggages\n",
    "                'wh_fleg_baggage_pieces':'number_checked_luggages',\n",
    "\n",
    "                # Passengers\n",
    "                'wh_fleg_pax_fln_e':  'premium_eco_passengers',\n",
    "                'wh_fleg_pax_fln_f' : 'first_passengers',\n",
    "                'wh_fleg_pax_fln_c' : 'business_passengers',\n",
    "                'wh_fleg_pax_fln_y' : 'eco_passengers',\n",
    "\n",
    "                # Airport\n",
    "                'flights_per_day':'flights_per_day',\n",
    "                'previous_is_delayed':'previous_is_delayed',\n",
    "\n",
    "                \n",
    "                # LABELS\n",
    "                'wh_fdel_delay_time':'delay_minutes',\n",
    "                'is_delayed':'delayed',\n",
    "                }\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = merged_df.rename(columns=map_columns)\n",
    "merged_df = merged_df[map_columns.values()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigate relationship between these features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.id_flight = merged_df.id_flight.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's study some correlations between numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#pd.set_option('display.max_columns', None)\n",
    "#old_merged.corr(numeric_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.corr(numeric_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's interesting to see that there's nice correlation between CARGO DATA, NUMBER CHECKED BAGS (AND THEIR WEIGHTS), AND PASSENGER TYPE SEAT.\n",
    "This makes sense because the longer the trip, intuitively it's more likely that people will bring a checked bag and will purchase first/business class.\n",
    "\n",
    "Also, it interesting to see that (as expected) weight_checked_luggages and number_checked_luggages is correlated. We want to remove this since it might uniquely introduce MULTICOLLINEARITY.\n",
    "\n",
    "It doesnt' seem, tho, that there is a direct correlation between these features and delay minutes. This makes sense because delay is the result of a combination of factors, and not uniquely dependet on this factor.\n",
    "\n",
    "Maybe, introducing the weather information can give us more information about it. But let's check whether most of the delays are indeed caused by it. We can use the delay dataset and AITA codes to map the codes to delay.\n",
    "\n",
    "It seems that it's not the weather is not the most  common cause of delay. Besides the code 0 which is specific to the airline, the other code 93 referes to aircraft rotation, late arrivial from another flight.\n",
    "The other common cause is ATC capacity: Air Traffic Management System is its ability to provide Air navigation Services to a certain volume of air traffic, in line with the targeted high level of safety and without imposing significant operational, economic or environmental penalties under normal circumstances\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_delay.groupby(['wh_fdel_delay_code','wh_fdel_delay_subcode']).count().loc[:, ['wh_fdel_leg_i']].sort_values(by = 'wh_fdel_leg_i', ascending=False).head(10)\n",
    "\n",
    "# 0 (internal err) -> Not easy to understand what is is\n",
    "# 93 (RA): Aircraft rotation, late arrival of aircraft from another flight or previous sector\n",
    "# 81 (AT): ATC restriction en-route or capacity: aircraft on IFR flight plans, generally by centers, when these aircraft are operating between departure and destination terminal areas.\n",
    "# 89 (AM): Restrictions at airport of departure,airport/runway closed due obstruction, industrial action, staff shortage, political unrest, noise abatemen\n",
    "# 91 (RL): Passenger or Load Connection, awaiting load or passengers from another flight. Protection of stranded passengers onto a new flight.\n",
    "# 83 (AE): ATC restriction at destination\n",
    "# 85 (AS): Mandatory security\n",
    "# 84 (AW): ATC restriction due to weather at destination\n",
    "# 16 (PS): Commercial Publicity, Passenger Convenience, VIP, Press, Ground meals and missing personal items\n",
    "# 2 Delay codes starting with 2 (cargo/mail)\n",
    "# 41 (TD): Aircraft defects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's study the relationship between categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = merged_df.select_dtypes(include=['object']).iloc[:,1:].columns\n",
    "categorical_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x=categorical_columns[0], y='delay_minutes', data=merged_df) # WE REMOVE DC (THERE's NO delay data -> undersample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.season_trip = merged_df.season_trip.str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(y='delay_minutes', x='season_trip', data=merged_df, orient='v', showfliers = False) # doesn't seem to exists a direct link \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Origin- Destination airports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_common_airports_origin = list(merged_df.groupby('origin').count().sort_values(by='id_flight',ascending=False).index[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(rc={'figure.figsize':(20.7,10.27)})\n",
    "sns.boxplot(y='delay_minutes', x =categorical_columns[1], data=merged_df.where(merged_df['origin'].isin(most_common_airports_origin)), orient='v', showfliers = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_common_airports_origin = list(merged_df.groupby('destination').count().sort_values(by='id_flight',ascending=False).index[:30])\n",
    "sns.set(rc={'figure.figsize':(20.7,10.27)})\n",
    "sns.boxplot(y='delay_minutes', x ='destination', data=merged_df.where(merged_df['destination'].isin(most_common_airports_origin)), orient='v', showfliers = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like there's a relationship between the airports of origing and destination and the delay (it makes sense)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Include information about time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df['day_of_week'] = merged_df.scheduled_time_departure.apply(lambda x: pd.to_datetime(x).day_of_week)\n",
    "merged_df['day_of_year'] = merged_df.scheduled_time_departure.apply(lambda x: pd.to_datetime(x).day_of_year)\n",
    "merged_df['month'] = merged_df.scheduled_time_departure.apply(lambda x: pd.to_datetime(x).month)\n",
    "merged_df['year'] = merged_df.scheduled_time_departure.apply(lambda x: pd.to_datetime(x).year)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove unuseful variables\n",
    "- season_trip\n",
    "- year (you don't want introduce bias)\n",
    "- weight_checked_luggages ( multicollinearity)\n",
    "- scheduled_time_departure\n",
    "- carrier = DC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = merged_df.drop(columns=['season_trip','id_flight'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "combine total_number_passenger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df['total_number_passengers'] = merged_df['first_passengers'] + merged_df['business_passengers'] + merged_df['eco_passengers'] + merged_df['premium_eco_passengers']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = merged_df.drop(columns=['first_passengers','eco_passengers','business_passengers','premium_eco_passengers'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove cancelled flights to downsample majority class\n",
    "merged_df = merged_df[~cancelled_flights_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remvoe carrier DC to downsample mahority class\n",
    "merged_df = merged_df[merged_df.carrier !='DC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove duplicate entries\n",
    "merged_df.duplicated().sum() # 8604\n",
    "merged_df = merged_df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.isna().sum() # check nan values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#map label to 0 and 1\n",
    "merged_df.delayed.unique()\n",
    "merged_df['delayed'] = merged_df['delayed'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_df = merged_df[[x for x in merged_df.columns if x !='delayed']+['delayed']] # add label at the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_df.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_df[cleaned_df.distance_trip == 0].groupby(by='delayed').count() # turn around flight (SHALL I REMOVE IT) ==> I add info in the db itself\n",
    "\n",
    "cleaned_df['is_turnaround']  = (cleaned_df['distance_trip'] == 0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaned_df = cleaned_df[[x for x in cleaned_df.columns if x !='delayed']+['delayed']]\n",
    "cleaned_df.previous_is_delayed = cleaned_df.previous_is_delayed.astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights_inbetween = pd.read_csv('/Users/favea/Downloads/swiss-data/num_flights_within_hour.csv')\n",
    "flights_inbetween.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with wandb.init(project='swiss-delay-prediction', entity=None, job_type=\"processed-dataset\") as run:\n",
    "    table_merged = wandb.Table(dataframe=cleaned_df)\n",
    "\n",
    "\n",
    "    # Create an artifact for our dataset\n",
    "    dataset_artifact = wandb.Artifact(\n",
    "        'dataset-cleaned', type='dataset',\n",
    "        description='Table containing the cleaned dataset that can be use for training',\n",
    "    )\n",
    "    # Add the table to the artifact & log the artifact\n",
    "    dataset_artifact.add(table_merged, 'data-table-delay-cleaned')\n",
    "\n",
    "\n",
    "    # Add the \n",
    "    run.log_artifact(dataset_artifact)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split dataset into train,validation, and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_full, test = train_test_split(cleaned_df.values, test_size=.10,  shuffle = True, random_state=123456,stratify= cleaned_df.values[:,-1])\n",
    "train, dev = train_test_split(train_full, test_size=.20,  shuffle = True, random_state=123456,stratify= train_full[:,-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train), len(dev), len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame(train, columns=cleaned_df.columns)\n",
    "dev_df = pd.DataFrame(dev, columns=cleaned_df.columns)\n",
    "test_df = pd.DataFrame(test, columns=cleaned_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_path = '/Users/favea/Downloads/swiss-data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_df.to_csv(os.path.join(out_path,'cleaned_df.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv(os.path.join(out_path,'train_df.csv'))\n",
    "dev_df.to_csv(os.path.join(out_path,'dev_df.csv'))\n",
    "test_df.to_csv(os.path.join(out_path,'test_df.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with wandb.init(project='swiss-delay-prediction', entity=None, job_type=\"train-dev-test-split\") as run:\n",
    "    \n",
    "\n",
    "    artifact = wandb.Artifact('stratified_split', type='dataset')\n",
    "    artifact.add_file('/Users/favea/Downloads/swiss-data/train_df.csv')\n",
    "    artifact.add_file('/Users/favea/Downloads/swiss-data/dev_df.csv')\n",
    "    artifact.add_file('/Users/favea/Downloads/swiss-data/test_df.csv')\n",
    "\n",
    "    run.log_artifact(artifact)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('gzaff')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "280bb2b5bdab4e32d0e32ca5e48973259a6d8e2f8db8c3a8a31b6fc317ce4513"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
